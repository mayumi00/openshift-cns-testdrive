= インフラストラクチャノード と Operator
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:

== 演習の概要
OpenShiftのサブスクリプションモデルでは、顧客は追加料金なしで様々なコアインフラストラクチャコンポーネントをを実行できます。つまり、OpenShiftのコアインフラストラクチャコンポーネントのみを実行しているノードは、クラスター環境をカバーするために必要なサブスクリプションの総数にはカウントされません。

インフラストラクチャのカテゴライズに該当するOpenShiftコンポーネントは以下が含まれます。

* kubernetesとOpenShiftのコントロールプレーンサービス（"masters"）。
* ルータ
* コンテナイメージレジストリ
* クラスタメトリクスの収集 ("monitoring")
* クラスタ集約型ロギング
* サービスブローカー

上記以外のコンテナ/Pod/コンポーネントを実行しているノードはすべてワーカーとみなされ、サブスクリプションでカバーされている必要があります。

---

### MachineSet 詳細
`MachineSets` の演習では、`MachineSets` を使用して、レプリカ数を変更してクラスタをスケーリングすることを検討しました。インフラストラクチャノードの場合、特定のKubernetesラベルを持つ `Machine` を追加で作成したいと思います。そして、それらのラベルを持つノード上で特定の動作をするように様々なインフラストラクチャコンポーネントを設定することができます。

[Note]
====
現在、インフラストラクチャコンポーネントの制御に使用されているOperatorは、"taint" と "toleration" の使用をすべてサポートしているわけではありません。これは、インフラストラクチャのワークロードはインフラストラクチャノード上で実行されますが、他のワークロードがインフラストラクチャノード上で実行されることは特に禁止されていないことを意味します。言い換えれば、すべてのOperatorに taint/toleration が完全に実装されるまでは、ユーザワークロードとインフラストラクチャワークロードが混在する可能性があります。

taint/tolerationの使用は、この演習ではカバーされていません。
====

これを実現するために、`MachineSets` を追加で作成します。

`MachineSets` がどのように動作するかを理解するために、手順を進めていきましょう。

[source,bash,role="execute"]
----
CLUSTERNAME=$(oc get infrastructure cluster -o=jsonpath='{.status.infrastructureName}')
ZONENAME=$(oc get nodes --sort-by='.metadata.creationTimestamp' -o=jsonpath='{.items[-1:].metadata.labels.topology\.kubernetes\.io/zone}')
oc get machineset.machine.openshift.io -n openshift-machine-api -o yaml ${CLUSTERNAME}-worker-${ZONENAME}
----

#### Metadata
`MachineSet`  の `metadata` には、`MachineSet` の名前や、様々なラベルのような情報が含まれています。


```YAML
metadata:
  annotations:
    machine.openshift.io/GPU: "0"
    machine.openshift.io/memoryMb: "65536"
    machine.openshift.io/vCPU: "16"
  creationTimestamp: "2023-11-06T00:31:02Z"
  generation: 3
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-q5kr6-w97nj
  name: cluster-q5kr6-w97nj-worker-ap-southeast-1c
  namespace: openshift-machine-api
  resourceVersion: "160525"
  uid: 4f4368dc-7e20-40d4-aad6-e445b1d23179
```

[Note]
====
`MachineAutoScaler` が定義されている `MachineSet` をダンプした場合、`MachineSet` に `annotation` が表示されるかもしれません。
====

#### Selector
`MachineSet` は `Machine` の作成方法を定義し、`Selector` はどのマシンがそのセットに関連付けられているかをOperatorに指示します。

```YAML
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-q5kr6-w97nj
      machine.openshift.io/cluster-api-machineset: cluster-q5kr6-w97nj-worker-ap-southeast-1c
```

この場合、クラスタ名は `cluster-q5kr6-w97nj` であり、セット全体のラベルが追加されています。

### Template Metadata
`template` は、`MachineSet` の一部で、`Machine` をテンプレート化するものです。

```YAML
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-q5kr6-w97nj
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: cluster-q5kr6-w97nj-worker-ap-southeast-1c
```

#### Template Spec
`template` は、`Machine`/`Node` をどのように作成するかを指定する必要があります。
`spec`、より具体的には、`providerSpec` には、`Machine` を正しく作成してブートストラップするための重要なAWSデータがすべて含まれていることに気づくでしょう。

この例では、結果として得られるノードが1つ以上の特定のラベルを継承していることを確認したいと思います。上の例で見たように、ラベルは `metadata` セクションにあります。
```YAML
    spec:
      lifecycleHooks: {}
      metadata: {}
      providerSpec:
        value:
          ami:
            id: ami-0f827a1be73b9de83
...
```

デフォルトでは、インストーラが作成する `MachineSets` は、ノードに追加のラベルを適用しません。

### カスタムMachineSetの定義
既存の `MachineSet` を分析したところで、次は作成のルールを確認してみましょう。

1. `providerSpec` の中では何も変更しない
2. `machine.openshift.io/cluster-api-cluster: <clusterid>` のインスタンスを変更しない
3. `MachineSet` にユニークな `name` を指定する
4. `machine.openshift.io/cluster-api-machineset` のインスタンスが `name` と一致することを確認する
5. ノードに必要なラベルを `.spec.template.spec.metadata.labels` に追加する
6. `MachineSet` `name` の参照を変更する場合でも、`subnet` を変更しないように注意する

一見複雑に見えますが、以下のように実行してみましょう。

WARNING: 踏み台ホストにログインしている場合は、ログアウトして実行してください。

[source,bash,role="execute"]
----
bash {{ HOME_PATH }}/support/machineset-generator.sh 1 infra 0 | oc create -f -
bash {{ HOME_PATH }}/support/machineset-patch.sh
export MACHINESET=$(oc get machineset.machine.openshift.io -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')
oc scale machineset.machine.openshift.io/$MACHINESET -n openshift-machine-api --replicas=3
----

次のように実行します。

[source,bash,role="execute"]
----
oc get machineset.machine.openshift.io -n openshift-machine-api
----

新しいインフラセットが以下例に似た名前で表示されているはずです。

```
...
cluster-q5kr6-w97nj-infra-ap-southeast-1a    3         3                             23s
...
```

まだインスタンスが起動していてブートストラップを行っているため、セットの中には利用可能なマシンがありません。
インスタンスがいつ実行されるかは `oc get machine -n openshift-machine-api` で確認することができます。
次に `oc get node` を使って、実際のノードがいつ結合されて準備が整ったかを確認することができます。

[Note]
====
`Machine` が準備されて `Node` として追加されるまでには数分かかることがあります。
====

[source,bash,role="execute"]
----
oc get nodes
----

```
NAME                                              STATUS   ROLES                  AGE    VERSION
ip-10-0-130-208.ap-southeast-1.compute.internal   Ready    infra,worker           52s    v1.25.14+20cda61
ip-10-0-135-241.ap-southeast-1.compute.internal   Ready    worker                 124m   v1.25.14+20cda61
ip-10-0-136-214.ap-southeast-1.compute.internal   Ready    infra,worker           51s    v1.25.14+20cda61
ip-10-0-140-52.ap-southeast-1.compute.internal    Ready    infra,worker           49s    v1.25.14+20cda61
ip-10-0-142-79.ap-southeast-1.compute.internal    Ready    control-plane,master   134m   v1.25.14+20cda61
ip-10-0-160-234.ap-southeast-1.compute.internal   Ready    worker                 124m   v1.25.14+20cda61
ip-10-0-171-226.ap-southeast-1.compute.internal   Ready    control-plane,master   134m   v1.25.14+20cda61
ip-10-0-205-200.ap-southeast-1.compute.internal   Ready    control-plane,master   134m   v1.25.14+20cda61
ip-10-0-216-253.ap-southeast-1.compute.internal   Ready    worker                 22m    v1.25.14+20cda61
```

どのノードが新しいノードなのか分からなくて困っている場合は、`AGE` カラムを見てみてください。
また、`ROLES` 列では、新しいノードが `worker` と `infra` の両方のロールを持っていることに気づくでしょう。

[source,bash,role="execute"]
----
oc get nodes -l node-role.kubernetes.io/infra
----

### ラベルを確認する
この例では、一番若いノードは `ip-10-0-130-208.ap-southeast-1.compute.internal` という名前でした。

[source,bash,role="execute"]
----
YOUNG_INFRA_NODE=$(oc get nodes -l node-role.kubernetes.io/infra  --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[0].metadata.name}')
oc get nodes ${YOUNG_INFRA_NODE} --show-labels | grep --color node-role
----

そして、`LABELS` の欄には、次のように書かれています。

```
beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.4xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=ap-southeast-1,failure-domain.beta.kubernetes.io/zone=ap-southeast-1a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-130-208.ap-southeast-1.compute.internal,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.kubernetes.io/instance-type=m5.4xlarge,node.openshift.io/os_id=rhcos
```

`node-role.kubernetes.io/infra` ラベルが確認できます。

### MachineSetの追加(スケール)
現実的な本番環境では、インフラストラクチャコンポーネントを保持するために、少なくとも3つの `MachineSets` が必要になります。ロギングアグリゲーションソリューションとサービスメッシュの両方がElasticSearchをデプロイするので、ElasticSearchは3つのノードに分散した3つのインスタンスを必要とします。なぜ3つの `MachineSets` が必要なのでしょうか。理論的には、異なるAZに複数の `MachineSets` を配置することで、AWSがAZを失った場合であっても完全にダウンすることを防ぐためです。

スクリプトレットで作成した `MachineSet` はすでに3つのレプリカを作成しているので、今のところ何もする必要はありません。また、自分で追加のレプリカを作成する必要もありません。

### 追加クレジット
`openshift-machine-api` プロジェクトにはいくつかの `Pods` があります。そのうちの一つは `machine-api-controllers-56bdc6874f-86jnb` のような名前です。その `Pod` のコンテナ上で `oc log` を使うと、ノードを実際に生成するためのさまざまなOperatorのビットを見ることができます。

```
(例)
oc logs machine-api-controllers-56bdc6874f-86jnb -c machine-controller -n openshift-machine-api
```

## Operatorの背景
Operatorはただの `Pods` です。しかし 彼らは特別な `Pods` であり、Kubernetes環境でアプリケーションをデプロイして管理する方法を理解しているソフトウェアです。Operatorのパワーは、`CustomResourceDefinitions` (`CRD`)と呼ばれるKubernetesの機能に依存しています。`CRD` はまさにその名の通りの機能です。これらはカスタムリソースを定義する方法であり、本質的にはKubernetes APIを新しいオブジェクトで拡張するものです。

Kubernetesで `Foo` オブジェクトを作成/読み込み/更新/削除できるようにしたい場合、`Foo` リソースとは何か、どのように動作するのかを定義した `CRD` を作成します。そして、`CRD` のインスタンスである `CustomResources` (`CRs`) を作成することができます。

Operator の場合、一般的なパターンとしては、Operator が `CRs` を見て設定を行い、Kubernetes 環境上で _operate_ を行い、設定で指定されたことを実行するというものです。ここでは、OpenShiftのインフラストラクチャオペレータのいくつかがどのように動作するかを見てみましょう。

## インフラストラクチャコンポーネントの移動
これで特別なノードができたので、インフラストラクチャのコンポーネントをその上に移動させることができます。

### ルータ
OpenShiftルータは `openshift-ingress-operator` という `Operator` によって管理されています。その `Pod` は `openshift-ingress-operator` プロジェクトに存在します。

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress-operator
----

実際のデフォルトのルータのインスタンスは `openshift-ingress` プロジェクトにあります。 `Pods` を見てみましょう。

[source,bash,role="execute"]
----
oc get pods -n openshift-ingress -o wide
----

以下のように確認できます。

```
NAME                              READY   STATUS    RESTARTS   AGE    IP            NODE
                            NOMINATED NODE   READINESS GATES
router-default-775577dc75-dhwwn   1/1     Running   0          146m   10.131.0.67   ip-10-0-160-234.ap-southeast-1.compute.internal   <none>           <none>
router-default-775577dc75-pcmxk   1/1     Running   0          146m   10.128.2.97   ip-10-0-135-241.ap-southeast-1.compute.internal   <none>           <none>
```

ルータが動作している `Node` を確認します。

[source,bash,role="execute"]
----
ROUTER_POD_NODE=$(oc get pods -n openshift-ingress -o jsonpath='{.items[0].spec.nodeName}')
oc get node ${ROUTER_POD_NODE}
----

`worker` の役割が指定されていることが確認できます。

```
NAME                                              STATUS   ROLES    AGE    VERSION
ip-10-0-160-234.ap-southeast-1.compute.internal   Ready    worker   3h7m   v1.25.14+20cda61
```

ルータオペレータのデフォルトの設定では、`worker` の役割を持つノードを見つけてルータを配置するようになっています。しかし、専用のインフラストラクチャノードを作成したので、ルータインスタンスを `infra` の役割を持つノードに配置するようにオペレータに指示します。

OpenShiftのルーターオペレータは、`ingresses.config.openshift.io` という `CustomResourceDefinitions`(`CRD`)を使用して、クラスタのデフォルトルーティングサブドメインを定義します。

[source,bash,role="execute"]
----
oc get ingresses.config.openshift.io cluster -o yaml
----

`cluster` オブジェクトはmasterだけでなくルータオペレータにも観測されます。以下のようなyamlになるでしょう。

```YAML
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: "2023-11-06T00:30:07Z"
  generation: 1
  name: cluster
  resourceVersion: "67230"
  uid: 191cc9b5-7887-411d-b3c5-00558e2dfb6e
spec:
  domain: {{ ROUTE_SUBDOMAIN }}
status: {}
```

個々のルータのデプロイは `ingresscontrollers.operator.openshift.io` CRD で管理されます。
ネームスペース `openshift-ingress-operator` に作成されたデフォルトのものがあります。


[source,bash,role="execute"]
----
oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml
----

以下のようになります。

```YAML
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2023-11-06T00:36:44Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "64195"
  uid: fc2d20ec-f693-4ce2-880e-cbb289fe6ad4
spec:
  clientTLS:
    clientCA:
      name: ""
    clientCertificatePolicy: ""
  defaultCertificate:
    name: ingress-certs-2023-11-06
  httpCompression: {}
  httpEmptyRequestsPolicy: Respond
  httpErrorCodePages:
    name: ""
  replicas: 2
  tuningOptions:
    reloadInterval: 0s
  unsupportedConfigOverrides: null
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2023-11-06T00:36:49Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2023-11-06T00:42:53Z"
    status: "True"
    type: PodsScheduled
  - lastTransitionTime: "2023-11-06T00:43:24Z"
    message: The deployment has Available status condition set to True
    reason: DeploymentAvailable
    status: "True"
    type: DeploymentAvailable
  - lastTransitionTime: "2023-11-06T00:43:24Z"
    message: Minimum replicas requirement is met
    reason: DeploymentMinimumReplicasMet
    status: "True"
    type: DeploymentReplicasMinAvailable
  - lastTransitionTime: "2023-11-06T01:23:02Z"
    message: All replicas are available
    reason: DeploymentReplicasAvailable
    status: "True"
    type: DeploymentReplicasAllAvailable
  - lastTransitionTime: "2023-11-06T01:23:02Z"
    message: Deployment is not actively rolling out
    reason: DeploymentNotRollingOut
    status: "False"
    type: DeploymentRollingOut
  - lastTransitionTime: "2023-11-06T00:36:50Z"
    message: The endpoint publishing strategy supports a managed load balancer
    reason: WantedByEndpointPublishingStrategy
    status: "True"
    type: LoadBalancerManaged
  - lastTransitionTime: "2023-11-06T00:41:20Z"
    message: The LoadBalancer service is provisioned
    reason: LoadBalancerProvisioned
    status: "True"
    type: LoadBalancerReady
  - lastTransitionTime: "2023-11-06T00:36:50Z"
    message: LoadBalancer is not progressing
    reason: LoadBalancerNotProgressing
    status: "False"
    type: LoadBalancerProgressing
  - lastTransitionTime: "2023-11-06T00:36:50Z"
    message: DNS management is supported and zones are specified in the cluster DNS
      config.
    reason: Normal
    status: "True"
    type: DNSManaged
  - lastTransitionTime: "2023-11-06T00:41:43Z"
    message: The record is provisioned in all reported zones.
    reason: NoFailedZones
    status: "True"
    type: DNSReady
  - lastTransitionTime: "2023-11-06T00:43:24Z"
    status: "True"
    type: Available
  - lastTransitionTime: "2023-11-06T01:23:02Z"
    status: "False"
    type: Progressing
  - lastTransitionTime: "2023-11-06T00:43:39Z"
    status: "False"
    type: Degraded
  - lastTransitionTime: "2023-11-06T00:36:50Z"
    message: IngressController is upgradeable.
    reason: Upgradeable
    status: "True"
    type: Upgradeable
  - lastTransitionTime: "2023-11-06T00:36:50Z"
    message: No evaluation condition is detected.
    reason: NoEvaluationCondition
    status: "False"
    type: EvaluationConditionsDetected
  - lastTransitionTime: "2023-11-06T00:43:39Z"
    message: Canary route checks for the default ingress controller are successful
    reason: CanaryChecksSucceeding
    status: "True"
    type: CanaryChecksSucceeding
  domain: apps.cluster-q5kr6.q5kr6.sandbox6.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
```

ルータPodがインフラストラクチャノードにヒットするように指示する `nodeSelector` を指定するには、以下の設定を適用します。

[source,bash,role="execute"]
----
oc apply -f {{ HOME_PATH }}/support/ingresscontroller.yaml
----

`Warning: resource is missing the kubectl.kubernetes.io/last-applied-config` のようなエラーが表示されるかもしれません。これは正常で、applyを実行すると、リソースに対して "3 way diff merge" が実行されます。ingress controllerはインストール時に作成されたばかりなので、 "last applied" configurationはありません。このコマンドを再実行すると、この警告は表示されなくなるはずです。

実行:

[source,bash,role="execute"]
----
oc get pod -n openshift-ingress -o wide
----

[Note]
====
ルーターの移動中にセッションがタイムアウトすることがあります。
ページを更新してセッションを取り戻してください。
端末セッションが失われることはありませんが、手動でこのページに戻る必要があるかもしれません。
====

もし十分に手際が良ければ、`Terminating` か `ContainerCreating` のいずれかのPodを捕まえることができるかもしれません。
`Terminating` Podはワーカーノードの1つで動作していました。
実行中の `Running` Podは最終的に `infra` ロールを持つノードの1つで動作しています。

## レジストリ
レジストリは、オペレータが実際のレジストリPodをどのように展開するかを設定するために、同様の `CRD` メカニズムを使用します。
このCRDは `configs.imageregistry.operator.openshift.io` です。
このCRDに `nodeSelector` を追加するために `cluster` のCRDオブジェクトを編集します。まず、それを見てみましょう。

[source,bash,role="execute"]
----
oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
----

以下のように確認できます。

```YAML
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2023-11-06T00:42:40Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 2
  name: cluster
  resourceVersion: "189466"
  uid: eca21c44-1879-4928-b1b9-69b890fbda59
spec:
  httpSecret: e15e7536849d6ffd15371669f4ec943e9e29824321e24b3ea639ca26d34bc2a9808d21e197e75a57cde491ca8f66
5612d1204cb3b5187b5254cb9ebd5c85f27d
  logLevel: Normal
  managementState: Managed
  observedConfig: null
  operatorLogLevel: Normal
  proxy: {}
  replicas: 2
  requests:
    read:
      maxWaitInQueue: 0s
    write:
      maxWaitInQueue: 0s
  rolloutStrategy: RollingUpdate
  storage:
    managementState: Managed
    s3:
      bucket: cluster-q5kr6-w97nj-image-registry-ap-southeast-1-uxuigubninuh
      encrypt: true
      region: ap-southeast-1
      trustedCA:
        name: ""
      virtualHostedStyle: false
  unsupportedConfigOverrides: null
status:
...
```

次のコマンドを実行します。

[source,bash,role="execute"]
----
oc patch configs.imageregistry.operator.openshift.io/cluster -p '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra": ""}}}' --type=merge
----

上記コマンドによって、レジストリCRの `.spec` を修正し、`nodeSelector` を追加します。

[Note]
====
この時点では、イメージレジストリはOperatorのために別のプロジェクトを使用していません。
Operatorとオペランドは両方とも `openshift-image-registry` プロジェクトの中にあります。
====

パッチコマンドを実行すると、レジストリPodがinfraノードに移動しているのがわかるはずです。
レジストリは `openshift-image-registry` プロジェクトにあります。

以下を素早く実行してみてください。


[source,bash,role="execute"]
----
oc get pod -n openshift-image-registry
----

古いレジストリPodが終了し、新しいレジストリPodが起動しているのがわかるかもしれません。
レジストリはS3バケットによってバックアップされているので、新しいレジストリPodのインスタンスがどのノードにあるかは問題ではありません。
これはAPI経由でオブジェクトストアと通信しているので、そこに保存されている既存のイメージはすべてアクセス可能なままです。

また、デフォルトのレプリカ数は1であることにも注意してください。
現実の環境では、可用性やネットワークのスループットなどの理由から、このレプリカ数を増やしたいと思うかもしれません。

レジストリが着地したノード(ルータのセクションを参照)を見てみると、それが現在infraワーカー上で実行されていることに気づくでしょう。

最後に、イメージレジストリの設定のための `CRD` がネームスペースではなく、クラスタスコープになっていることに注目してください。
OpenShiftクラスタごとに内部/統合レジストリは1つしかありません。

## Monitoring
Cluster Monitoring operatorは、Prometheus+Grafana+AlertManagerによるクラスタ監視スタックの展開と状態管理を担当します。これは、クラスタの初期インストール時にデフォルトでインストールされます。このオペレータは `openshift-monitoring` プロジェクトの `ConfigMap` を利用して、監視スタックの動作のために様々なチューニングや設定を行います。

以下の `ConfigMap` 定義は、インフラストラクチャノードにデプロイされる監視ソリューションを設定するものです。


```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
```

インストールの一部として作成された `ConfigMap` は存在しません。これがない場合、Operatorはデフォルトの設定を仮定します。
クラスタに `ConfigMap` が定義されていないことを確認してください。

[source,bash,role="execute"]
----
oc get configmap cluster-monitoring-config -n openshift-monitoring
----

以下のように出力されるはずです。

```
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
```


NOTE: cluster-monitoring-configがある場合は、以下のコマンドを使用してコンフィギュレーションを削除することをお勧めします。

[source,bash,role="execute"]
----
oc delete configmap cluster-monitoring-config -n openshift-monitoring
----

Operatorは、さまざまな監視スタック コンポーネントに対して複数の `ConfigMap` オブジェクトを作成し、それらを表示できます。

[source,bash,role="execute"]
----
oc get configmap -n openshift-monitoring
----

次のコマンドで新しいモニタリング設定を作成できます。

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/support/cluster-monitoring-configmap.yaml
----

モニタリングPodが `worker` から `infra` `Nodes` に移動するのを見てみましょう。

[source,bash,role="execute"]
----
watch 'oc get pod -n openshift-monitoring'
----

または

[source,bash,role="execute"]
----
oc get pod -w -n openshift-monitoring
----
kbd:[Ctrl+C]を押すと終了できます。

## Logging
OpenShiftのログ集約ソリューションはデフォルトではインストールされていません。
ロギングの設定とデプロイメントを行う専用のラボ演習があります。
